{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "difficult-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ExponentialLR, CosineAnnealingLR, StepLR, OneCycleLR\n",
    "\n",
    "from dataloader import Dataset_AMEX\n",
    "from metric import AmexMetric\n",
    "from model_kaggle_ben import Transformer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reasonable-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Dataset_AMEX('val')\n",
    "# y_true = torch.tensor(dataset[0][1], dtype=torch.float)[None]\n",
    "\n",
    "# model = Transformer(num_tokens=1,\n",
    "#         feat_dim=188,\n",
    "#         embed_dim = 64,\n",
    "#         num_heads=4,\n",
    "#         num_encoder_layers=2,\n",
    "#         dropout_p=0.3)\n",
    "# y_hats = model(torch.tensor(dataset[0][0])[None])\n",
    "# loss_fn = nn.BCELoss(reduction=\"mean\")\n",
    "# loss_fn(y_hats.squeeze(1), y_true)\n",
    "# #val_amex_metric = AmexMetric()\n",
    "# #val_amex_metric.update(y_hats.reshape(-1), y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-spoke",
   "metadata": {},
   "source": [
    "# Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "infectious-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_pl(pl.LightningDataModule):\n",
    "    def __init__(self, fold):\n",
    "        super().__init__()\n",
    "        self.fold = 1\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage= None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_set = Dataset_AMEX('train', fold=self.fold)\n",
    "            self.val_set = Dataset_AMEX('val', fold=self.fold)\n",
    "        if stage == \"validate\":\n",
    "            self.val_set = Dataset_AMEX('val', fold=self.fold)\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.val_set = Dataset_AMEX('val', fold=self.fold)\n",
    "        if stage == \"predict\" or stage is None:\n",
    "            self.test_set = Dataset_AMEX('test')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=512, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=2048, shuffle=False, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=4096, shuffle=False, num_workers=4)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=4096, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-psychology",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mineral-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_transformer(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3):#, batch_size):\n",
    "        super().__init__()\n",
    "        self.model = Transformer(num_tokens=1,\n",
    "                        feat_dim=188,\n",
    "                        embed_dim = 64,\n",
    "                        num_heads=4,\n",
    "                        num_encoder_layers=2,\n",
    "                        dropout_p=0.3)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_amex_metric = AmexMetric()\n",
    "        self.val_amex_metric = AmexMetric()\n",
    "        self.loss_fn = nn.BCELoss(reduction=\"mean\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        y_hat = self.model(x)\n",
    "        # loss function\n",
    "        loss = self.loss_fn(y_hat.squeeze(1), y)\n",
    "        self.train_amex_metric.update(y_hat.squeeze(1), y)\n",
    "        self.log_dict({'train_loss': loss, 'train_amex_metric': self.train_amex_metric}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        y_hat = self.model(x)\n",
    "        # loss function\n",
    "        loss = self.loss_fn(y_hat.squeeze(1), y)\n",
    "        self.val_amex_metric.update(y_hat.squeeze(1), y)\n",
    "        self.log_dict({'val_loss': loss, 'val_amex_metric': self.val_amex_metric}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss': loss}       \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        y_hat = self.model(x)\n",
    "        # loss function\n",
    "        #loss = self.loss_fn(y_hats.squeeze(1), y_true)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.model(x)#.squeeze(1)\n",
    "        return y_hat\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        #lr_scheduler = OneCycleLR(optimizer, max_lr=1e-3, epochs=25, steps_per_epoch=718)\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=2, T_mult=3)\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-emergency",
   "metadata": {},
   "source": [
    "# Find LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "renewable-amendment",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dm = Dataset_pl(1)\n",
    "# model = Model_transformer()#, argv['batch_size']) \n",
    "# trainer = pl.Trainer(gpus=2, strategy='dp')\n",
    "# lr_finder = trainer.tuner.lr_find(model, dm)\n",
    "\n",
    "# # Results can be found in\n",
    "# lr_finder.results\n",
    "\n",
    "# # Plot with\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()\n",
    "\n",
    "# # Pick point based on plot, or get suggestion\n",
    "# new_lr = lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-mills",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "moderate-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm = Dataset_pl(1)\n",
    "# model = Model_transformer()#, argv['batch_size']) \n",
    "\n",
    "# wandb_logger = WandbLogger(project=\"AMEX\")\n",
    "# callbacks=[ModelCheckpoint(dirpath='ckpt', \n",
    "#                            monitor=\"val_amex_metric\", mode=\"max\")]\n",
    "\n",
    "# trainer = pl.Trainer(gpus=[1], max_epochs=30, \n",
    "#                     logger=wandb_logger, callbacks=callbacks,\n",
    "#                     enable_progress_bar=False)\n",
    "\n",
    "# trainer.fit(model, datamodule=dm)\n",
    "\n",
    "# # get validation metrics\n",
    "# val = trainer.validate(model, datamodule=dm, ckpt_path='best')\n",
    "# val_amex_metric_epoch = val[0]['val_amex_metric_epoch']\n",
    "\n",
    "# # get output\n",
    "# output = trainer.predict(model, datamodule=dm, ckpt_path='best')\n",
    "# output = torch.vstack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mighty-press",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training data shapes (367131, 13, 188) (367131,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type        | Params\n",
      "--------------------------------------------------\n",
      "0 | model             | Transformer | 509 K \n",
      "1 | train_amex_metric | AmexMetric  | 0     \n",
      "2 | val_amex_metric   | AmexMetric  | 0     \n",
      "3 | loss_fn           | BCELoss     | 0     \n",
      "--------------------------------------------------\n",
      "509 K     Trainable params\n",
      "0         Non-trainable params\n",
      "509 K     Total params\n",
      "2.037     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "Epoch 0:  31%|███       | 238/763 [00:10<00:22, 23.16it/s, loss=0.25, v_num=30, train_loss_step=0.271] "
     ]
    }
   ],
   "source": [
    "val_metrics = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    dm = Dataset_pl(i)\n",
    "    model = Model_transformer()#, argv['batch_size']) \n",
    "\n",
    "    #wandb_logger = WandbLogger(project=\"AMEX\")\n",
    "    callbacks=[ModelCheckpoint(dirpath='ckpt', \n",
    "                               monitor=\"val_amex_metric\", mode=\"max\")]\n",
    "\n",
    "#     trainer = pl.Trainer(gpus=[1], max_epochs=25, \n",
    "#                         logger=wandb_logger, callbacks=callbacks,\n",
    "#                         enable_progress_bar=True)\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=25, \n",
    "                         callbacks=callbacks,\n",
    "                         enable_progress_bar=True)\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "\n",
    "    # get validation metrics\n",
    "    val = trainer.validate(model, datamodule=dm, ckpt_path='best')\n",
    "    val_amex_metric_epoch = val[0]['val_amex_metric_epoch']\n",
    "    \n",
    "    # get output\n",
    "    output = trainer.predict(model, datamodule=dm, ckpt_path='best')\n",
    "    output = torch.vstack(output)\n",
    "    # save result\n",
    "    val_metrics.append(val_amex_metric_epoch)\n",
    "\n",
    "    df = pd.DataFrame(output)\n",
    "    df2 = pd.read_csv('submission.csv')\n",
    "    df2['prediction']=df.values\n",
    "    df2.to_csv(f'results/coslr_prediction_fold{i}.csv', index=False)\n",
    "    del dm, trainer, model, output, df, df2\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"fold {i}\", val_amex_metric_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-contractor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-orchestra",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-words",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-finish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-partnership",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-capability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-honor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6bc16ffe0f94954532e9b31506694ff5e12654e3b81ed4ee0ecf18a2ec59813b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fft')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
