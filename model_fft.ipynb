{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "established-combat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sexual-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()nvidia\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "    \n",
    "def a_norm(Q, K):\n",
    "    m = torch.matmul(Q, K.transpose(2, 1))\n",
    "    m /= torch.sqrt(torch.tensor(Q.shape[-1]).float())\n",
    "    return torch.softmax(m, -1)\n",
    "\n",
    "def attention(Q, K, V):\n",
    "    a = a_norm(Q, K)  # (batch_size, hidden_size, timestepgth)\n",
    "    return torch.matmul(a, V)  # (batch_size, timestepgth, timestepgth)\n",
    "\n",
    "class QKV(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(QKV, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_size, hidden_size, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, feat_dim, embed_dim):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.value = QKV(feat_dim, embed_dim)\n",
    "        self.key = QKV(feat_dim, embed_dim)\n",
    "        self.query = QKV(feat_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, kv=None):\n",
    "        if kv is None:\n",
    "            qkv = attention(self.query(x), self.key(x), self.value(x))\n",
    "            return qkv\n",
    "        else:\n",
    "            qkv = attention(self.query(x), self.key(kv), self.value(kv))\n",
    "            return qkv\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, feat_dim, embed_dim, num_heads):\n",
    "        '''input: embedding_size, output: embedding_size'''\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "        self.heads = []\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(AttentionBlock(feat_dim, embed_dim))\n",
    "        self.heads = nn.Sequential(*self.heads)\n",
    "        self.fc = nn.Linear(num_heads * embed_dim, feat_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x, kv=None):\n",
    "        a = []\n",
    "        for h in self.heads:\n",
    "            a.append(h(x, kv=kv))\n",
    "        a = torch.stack(a, dim=-1)  # combine heads\n",
    "        a = a.flatten(start_dim=2)  # flatten all head outputs\n",
    "        x = self.fc(a)\n",
    "        return x\n",
    "\n",
    "class Aug_FFT(nn.Module):\n",
    "    def __init__(self, feat_dim, embed_dim):\n",
    "        super(Aug_FFT, self).__init__()\n",
    "        self.value = QKV(feat_dim, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, feat_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.value(x)\n",
    "        x = self.fc(torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=128, feat_dim=188, num_heads=4, ff_dim=64, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttentionBlock(feat_dim, embed_dim, num_heads)\n",
    "        self.fft = Aug_FFT(feat_dim, embed_dim)\n",
    "        self.ffn = nn.Sequential(nn.Linear(feat_dim, ff_dim), nn.GELU(), nn.Linear(ff_dim, feat_dim))\n",
    "        self.ln1 = nn.LayerNorm([13, feat_dim], eps=1e-6)\n",
    "        self.ln2 = nn.LayerNorm([13, feat_dim], eps=1e-6)\n",
    "        self.dp1 = nn.Dropout(rate)\n",
    "        self.dp2 = nn.Dropout(rate)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        attn_output = self.att(inputs)  # inputs bs,13,188, output bs,13,188\n",
    "        fft_output = self.fft(inputs)  # inputs bs,13,188, output bs,13,188\n",
    "        attn_output = self.dp1(attn_output) \n",
    "        out1 = self.ln1(inputs+attn_output+fft_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dp2(ffn_output)\n",
    "        return self.ln2(out1+ffn_output)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_tokens=1,\n",
    "                        embed_dim=128,\n",
    "                        feat_dim=188,\n",
    "                        num_heads=4,\n",
    "                        ff_dim=64,\n",
    "                        num_encoder_layers=2,\n",
    "                        dropout_p=0.3):\n",
    "        super(Transformer, self).__init__()\n",
    "        # LAYERS\n",
    "        self.embeddings = {}\n",
    "        for k in range(11):\n",
    "            self.embeddings[k] = nn.Embedding(10, 4)#.to('cuda:1')\n",
    "        feat_dim2 = feat_dim-11+11*4\n",
    "        self.encoder = nn.Linear(feat_dim2, feat_dim) # bs,13,feat_dim\n",
    "\n",
    "        # Initiate encoder and Decoder layers\n",
    "        self.encs = []\n",
    "        for i in range(num_encoder_layers):\n",
    "            self.encs.append(TransformerBlock(embed_dim, feat_dim, num_heads, ff_dim, rate=0.1))\n",
    "        self.encs = nn.Sequential(*self.encs)\n",
    "\n",
    "#         # Dense layers for managing network inputs and outputs\n",
    "#         self.enc_input_fc = nn.Linear(feature, embedding_size)\n",
    "#         self.enc_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#         self.pos = PositionalEncoding(embedding_size)\n",
    "        self.out_fc1 = nn.Linear(feat_dim, 64)\n",
    "        self.out_fc2 = nn.Linear(64, 32)\n",
    "        self.out_fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # emb\n",
    "        src_cat = []\n",
    "        for k in range(11):\n",
    "            src_cat.append(self.embeddings[k](src[:,:,k].long())) #* math.sqrt(self.hidden_size)\n",
    "        src_cat = torch.cat(src_cat, -1)\n",
    "        src = torch.cat([src_cat, src[:,:,11:]], -1)\n",
    "        src = self.encoder(src)\n",
    "        \n",
    "        # encoder\n",
    "        for enc in self.encs:\n",
    "            src_old = src\n",
    "            src = enc(src)\n",
    "            src = 0.9*src + 0.1*src_old\n",
    "        src = F.elu(self.out_fc1(src[:,-1,:]))\n",
    "        src = F.elu(self.out_fc2(src))\n",
    "        src = torch.sigmoid(self.out_fc3(src))\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accepted-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Validation data shapes (91782, 13, 188) (91782,)\n"
     ]
    }
   ],
   "source": [
    "#inp = torch.randn(64,13,188)\n",
    "from dataloader import Dataset_AMEX\n",
    "ds = Dataset_AMEX('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pregnant-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(ds[0][0])[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "electric-crazy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 188])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "documentary-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(num_tokens=1,\n",
    "                        embed_dim=128,\n",
    "                        feat_dim=188,\n",
    "                        num_heads=4,\n",
    "                        ff_dim=64,\n",
    "                        num_encoder_layers=2,\n",
    "                        dropout_p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "committed-springfield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-breakdown",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-nickname",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-handy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-edmonton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-campbell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-milan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6bc16ffe0f94954532e9b31506694ff5e12654e3b81ed4ee0ecf18a2ec59813b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fft')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
