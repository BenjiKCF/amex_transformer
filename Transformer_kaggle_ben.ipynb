{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floppy-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ExponentialLR, CosineAnnealingLR, StepLR, OneCycleLR\n",
    "\n",
    "from dataloader import Dataset_AMEX\n",
    "from metric import AmexMetric\n",
    "from model_kaggle_ben import Transformer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smooth-verification",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dataset = Dataset_AMEX('val')\n",
    "# y_true = torch.tensor(dataset[0][1], dtype=torch.float)[None]\n",
    "\n",
    "# model = Transformer(num_tokens=1,\n",
    "#         feat_dim=188,\n",
    "#         embed_dim = 64,\n",
    "#         num_heads=4,\n",
    "#         num_encoder_layers=2,\n",
    "#         dropout_p=0.3)\n",
    "# y_hats = model(torch.tensor(dataset[0][0])[None])\n",
    "# loss_fn = nn.BCELoss(reduction=\"mean\")\n",
    "# loss_fn(y_hats.squeeze(1), y_true)\n",
    "# #val_amex_metric = AmexMetric()\n",
    "# #val_amex_metric.update(y_hats.reshape(-1), y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-darkness",
   "metadata": {},
   "source": [
    "# Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prerequisite-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_pl(pl.LightningDataModule):\n",
    "    def __init__(self, fold):\n",
    "        super().__init__()\n",
    "        self.fold = 1\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage= None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_set = Dataset_AMEX('train', fold=self.fold)\n",
    "            self.val_set = Dataset_AMEX('val', fold=self.fold)\n",
    "        if stage == \"validate\":\n",
    "            self.val_set = Dataset_AMEX('val', fold=self.fold)\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.val_set = Dataset_AMEX('val', fold=self.fold)\n",
    "        if stage == \"predict\" or stage is None:\n",
    "            self.test_set = Dataset_AMEX('test')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=512, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=2048, shuffle=False, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=4096, shuffle=False, num_workers=4)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=4096, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-colombia",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "personal-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_transformer(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3):#, batch_size):\n",
    "        super().__init__()\n",
    "        self.model = Transformer(num_tokens=1,\n",
    "                        feat_dim=188,\n",
    "                        embed_dim = 64,\n",
    "                        num_heads=4,\n",
    "                        num_encoder_layers=2,\n",
    "                        dropout_p=0.3)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_amex_metric = AmexMetric()\n",
    "        self.val_amex_metric = AmexMetric()\n",
    "        self.loss_fn = nn.BCELoss(reduction=\"mean\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        y_hat = self.model(x)\n",
    "        # loss function\n",
    "        loss = self.loss_fn(y_hat.squeeze(1), y)\n",
    "        self.train_amex_metric.update(y_hat.squeeze(1), y)\n",
    "        self.log_dict({'train_loss': loss, 'train_amex_metric': self.train_amex_metric}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        y_hat = self.model(x)\n",
    "        # loss function\n",
    "        loss = self.loss_fn(y_hat.squeeze(1), y)\n",
    "        self.val_amex_metric.update(y_hat.squeeze(1), y)\n",
    "        self.log_dict({'val_loss': loss, 'val_amex_metric': self.val_amex_metric}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss': loss}       \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        y_hat = self.model(x)\n",
    "        # loss function\n",
    "        #loss = self.loss_fn(y_hats.squeeze(1), y_true)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.model(x)#.squeeze(1)\n",
    "        return y_hat\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        lr_scheduler = OneCycleLR(optimizer, max_lr=1e-3, epochs=25, steps_per_epoch=718) #steps_per_epoch=len(dataloader)\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-cambridge",
   "metadata": {},
   "source": [
    "# Find LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "simple-adventure",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dm = Dataset_pl(1)\n",
    "# model = Model_transformer()#, argv['batch_size']) \n",
    "# trainer = pl.Trainer(gpus=2, strategy='dp')\n",
    "# lr_finder = trainer.tuner.lr_find(model, dm)\n",
    "\n",
    "# # Results can be found in\n",
    "# lr_finder.results\n",
    "\n",
    "# # Plot with\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()\n",
    "\n",
    "# # Pick point based on plot, or get suggestion\n",
    "# new_lr = lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-looking",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "integrated-kuwait",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbenjikcf\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cairs/code/amex/transformer/wandb/run-20220707_114526-3r0o3aqe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/benjikcf/AMEX/runs/3r0o3aqe\" target=\"_blank\">expert-monkey-64</a></strong> to <a href=\"https://wandb.ai/benjikcf/AMEX\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type        | Params\n",
      "--------------------------------------------------\n",
      "0 | model             | Transformer | 509 K \n",
      "1 | train_amex_metric | AmexMetric  | 0     \n",
      "2 | val_amex_metric   | AmexMetric  | 0     \n",
      "3 | loss_fn           | BCELoss     | 0     \n",
      "--------------------------------------------------\n",
      "509 K     Trainable params\n",
      "0         Non-trainable params\n",
      "509 K     Total params\n",
      "2.037     Total estimated model params size (MB)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=4'>5</a>\u001b[0m callbacks\u001b[39m=\u001b[39m[ModelCheckpoint(dirpath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mckpt\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=5'>6</a>\u001b[0m                            monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_amex_metric\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=7'>8</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_epochs\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=8'>9</a>\u001b[0m                     logger\u001b[39m=\u001b[39mwandb_logger, callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=9'>10</a>\u001b[0m                     enable_progress_bar\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=11'>12</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdm)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=13'>14</a>\u001b[0m \u001b[39m# get validation metrics\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=14'>15</a>\u001b[0m val \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mvalidate(model, datamodule\u001b[39m=\u001b[39mdm, ckpt_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbest\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=750'>751</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=751'>752</a>\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=752'>753</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=766'>767</a>\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=767'>768</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=768'>769</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=769'>770</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=770'>771</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=771'>772</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:723\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=720'>721</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=721'>722</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=722'>723</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=723'>724</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=724'>725</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:811\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=806'>807</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=807'>808</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=808'>809</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=809'>810</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=810'>811</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=812'>813</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=813'>814</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1236\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1231'>1232</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1233'>1234</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1235'>1236</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1237'>1238</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1238'>1239</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1323\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1320'>1321</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1321'>1322</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1322'>1323</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1345\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1341'>1342</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1343'>1344</a>\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1344'>1345</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1346'>1347</a>\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1347'>1348</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1413\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1410'>1411</a>\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1411'>1412</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1412'>1413</a>\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1414'>1415</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1416'>1417</a>\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:154\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=151'>152</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=152'>153</a>\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=153'>154</a>\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=155'>156</a>\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=156'>157</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:128\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=124'>125</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=126'>127</a>\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=127'>128</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=128'>129</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=130'>131</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:226\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=223'>224</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook(\u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39mkwargs\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=224'>225</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=225'>226</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mvalidation_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=227'>228</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1765\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1761'>1762</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1763'>1764</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1764'>1765</a>\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1766'>1767</a>\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1767'>1768</a>\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:344\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=338'>339</a>\u001b[0m \u001b[39m\"\"\"The actual validation step.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=339'>340</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=340'>341</a>\u001b[0m \u001b[39mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.validation_step` for more details\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=341'>342</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=342'>343</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py?line=343'>344</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb Cell 6'\u001b[0m in \u001b[0;36mModel_transformer.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000005vscode-remote?line=32'>33</a>\u001b[0m x, y \u001b[39m=\u001b[39m batch\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000005vscode-remote?line=33'>34</a>\u001b[0m x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mfloat(), y\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000005vscode-remote?line=34'>35</a>\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000005vscode-remote?line=35'>36</a>\u001b[0m \u001b[39m# loss function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000005vscode-remote?line=36'>37</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(y_hat\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m), y)\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/amex/transformer/model_kaggle_ben.py:143\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/code/amex/transformer/model_kaggle_ben.py?line=140'>141</a>\u001b[0m src_cat \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='file:///home/cairs/code/amex/transformer/model_kaggle_ben.py?line=141'>142</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m11\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/cairs/code/amex/transformer/model_kaggle_ben.py?line=142'>143</a>\u001b[0m     src_cat\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings[k](src[:,:,k]\u001b[39m.\u001b[39;49mlong())) \u001b[39m#* math.sqrt(self.hidden_size)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/code/amex/transformer/model_kaggle_ben.py?line=143'>144</a>\u001b[0m src_cat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(src_cat, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///home/cairs/code/amex/transformer/model_kaggle_ben.py?line=144'>145</a>\u001b[0m src \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([src_cat, src[:,:,\u001b[39m11\u001b[39m:]], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=156'>157</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=157'>158</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=158'>159</a>\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=159'>160</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/functional.py?line=2192'>2193</a>\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/functional.py?line=2193'>2194</a>\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/functional.py?line=2194'>2195</a>\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/functional.py?line=2195'>2196</a>\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/functional.py?line=2196'>2197</a>\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/functional.py?line=2197'>2198</a>\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/functional.py?line=2198'>2199</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "dm = Dataset_pl(1)\n",
    "model = Model_transformer()#, argv['batch_size']) \n",
    "\n",
    "wandb_logger = WandbLogger(project=\"AMEX\")\n",
    "callbacks=[ModelCheckpoint(dirpath='ckpt', \n",
    "                           monitor=\"val_amex_metric\", mode=\"max\")]\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=30, \n",
    "                    logger=wandb_logger, callbacks=callbacks,\n",
    "                    enable_progress_bar=False)\n",
    "\n",
    "trainer.fit(model, datamodule=dm)\n",
    "\n",
    "# get validation metrics\n",
    "val = trainer.validate(model, datamodule=dm, ckpt_path='best')\n",
    "val_amex_metric_epoch = val[0]['val_amex_metric_epoch']\n",
    "\n",
    "# get output\n",
    "output = trainer.predict(model, datamodule=dm, ckpt_path='best')\n",
    "output = torch.vstack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-combining",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training data shapes (367131, 13, 188) (367131,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type        | Params\n",
      "--------------------------------------------------\n",
      "0 | model             | Transformer | 509 K \n",
      "1 | train_amex_metric | AmexMetric  | 0     \n",
      "2 | val_amex_metric   | AmexMetric  | 0     \n",
      "3 | loss_fn           | BCELoss     | 0     \n",
      "--------------------------------------------------\n",
      "509 K     Trainable params\n",
      "0         Non-trainable params\n",
      "509 K     Total params\n",
      "2.037     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Validation data shapes (91782, 13, 188) (91782,)\n"
     ]
    }
   ],
   "source": [
    "# val_metrics = []\n",
    "# outputs = []\n",
    "# for i in range(1,11):\n",
    "#     dm = Dataset_pl(i)\n",
    "#     model = Model_transformer()#, argv['batch_size']) \n",
    "\n",
    "#     #wandb_logger = WandbLogger(project=\"AMEX\")\n",
    "#     callbacks=[ModelCheckpoint(dirpath='ckpt', \n",
    "#                                monitor=\"val_amex_metric\", mode=\"max\")]\n",
    "\n",
    "# #     trainer = pl.Trainer(gpus=[1], max_epochs=25, \n",
    "# #                         logger=wandb_logger, callbacks=callbacks,\n",
    "# #                         enable_progress_bar=False)\n",
    "#     trainer = pl.Trainer(gpus=[1], max_epochs=25, \n",
    "#                          callbacks=callbacks,\n",
    "#                          enable_progress_bar=False)\n",
    "#     trainer.fit(model, datamodule=dm)\n",
    "\n",
    "#     # get validation metrics\n",
    "#     val = trainer.validate(model, datamodule=dm, ckpt_path='best')\n",
    "#     val_amex_metric_epoch = val[0]['val_amex_metric_epoch']\n",
    "    \n",
    "#     # get output\n",
    "#     output = trainer.predict(model, datamodule=dm, ckpt_path='best')\n",
    "#     output = torch.vstack(output)\n",
    "#     # save result\n",
    "#     val_metrics.append(val_amex_metric_epoch)\n",
    "#     outputs.append(output)\n",
    "    \n",
    "#     print(f\"fold {i}\", val_amex_metric_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-dayton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-detector",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-programming",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-village",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-chess",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6bc16ffe0f94954532e9b31506694ff5e12654e3b81ed4ee0ecf18a2ec59813b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fft')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
