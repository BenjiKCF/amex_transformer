{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floppy-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ExponentialLR, CosineAnnealingLR, StepLR, OneCycleLR\n",
    "\n",
    "from dataloader import Dataset_AMEX\n",
    "from metric import AmexMetric\n",
    "from model_kaggle_ben import Transformer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smooth-verification",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dataset = Dataset_AMEX('val')\n",
    "# y_true = torch.tensor(dataset[0][1], dtype=torch.float)[None]\n",
    "\n",
    "# model = Transformer(num_tokens=1,\n",
    "#         feat_dim=188,\n",
    "#         embed_dim = 64,\n",
    "#         num_heads=4,\n",
    "#         num_encoder_layers=2,\n",
    "#         dropout_p=0.3)\n",
    "# y_hats = model(torch.tensor(dataset[0][0])[None])\n",
    "# loss_fn = nn.BCELoss(reduction=\"mean\")\n",
    "# loss_fn(y_hats.squeeze(1), y_true)\n",
    "# #val_amex_metric = AmexMetric()\n",
    "# #val_amex_metric.update(y_hats.reshape(-1), y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-darkness",
   "metadata": {},
   "source": [
    "# Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prerequisite-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_pl(pl.LightningDataModule):\n",
    "    def __init__(self, fold):\n",
    "        super().__init__()\n",
    "        self.fold = 1\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage= None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_set = Dataset_AMEX('train', fold=self.fold)\n",
    "            self.val_set = Dataset_AMEX('val', fold=self.fold)\n",
    "        if stage == \"validate\":\n",
    "            self.val_set = Dataset_AMEX('val', fold=self.fold)\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.val_set = Dataset_AMEX('val', fold=self.fold)\n",
    "        if stage == \"predict\" or stage is None:\n",
    "            self.test_set = Dataset_AMEX('test')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=512, shuffle=True, num_workers=1)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=2048, shuffle=False, num_workers=1)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=4096, shuffle=False, num_workers=1)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=4096, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-colombia",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "personal-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_transformer(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3):#, batch_size):\n",
    "        super().__init__()\n",
    "        self.model = Transformer(num_tokens=1,\n",
    "                        feat_dim=188,\n",
    "                        embed_dim = 64,\n",
    "                        num_heads=4,\n",
    "                        num_encoder_layers=2,\n",
    "                        dropout_p=0.3)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_amex_metric = AmexMetric()\n",
    "        self.val_amex_metric = AmexMetric()\n",
    "        self.loss_fn = nn.BCELoss(reduction=\"mean\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        y_hat = self.model(x)\n",
    "        # loss function\n",
    "        loss = self.loss_fn(y_hat.squeeze(1), y)\n",
    "        self.train_amex_metric.update(y_hat.squeeze(1), y)\n",
    "        self.log_dict({'train_loss': loss, 'train_amex_metric': self.train_amex_metric}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        y_hat = self.model(x)\n",
    "        # loss function\n",
    "        loss = self.loss_fn(y_hat.squeeze(1), y)\n",
    "        self.val_amex_metric.update(y_hat.squeeze(1), y)\n",
    "        self.log_dict({'val_loss': loss, 'val_amex_metric': self.val_amex_metric}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss': loss}       \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        y_hat = self.model(x)\n",
    "        # loss function\n",
    "        #loss = self.loss_fn(y_hats.squeeze(1), y_true)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.model(x)#.squeeze(1)\n",
    "        return y_hat\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        lr_scheduler = OneCycleLR(optimizer, max_lr=1e-3, epochs=25, steps_per_epoch=718) #steps_per_epoch=len(dataloader)\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-cambridge",
   "metadata": {},
   "source": [
    "# Find LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "simple-adventure",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dm = Dataset_pl(1)\n",
    "# model = Model_transformer()#, argv['batch_size']) \n",
    "# trainer = pl.Trainer(gpus=2, strategy='dp')\n",
    "# lr_finder = trainer.tuner.lr_find(model, dm)\n",
    "\n",
    "# # Results can be found in\n",
    "# lr_finder.results\n",
    "\n",
    "# # Plot with\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()\n",
    "\n",
    "# # Pick point based on plot, or get suggestion\n",
    "# new_lr = lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-looking",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "integrated-kuwait",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbenjikcf\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cairs/code/amex/transformer/wandb/run-20220707_112404-tp7kyf7l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/benjikcf/AMEX/runs/tp7kyf7l\" target=\"_blank\">copper-thunder-62</a></strong> to <a href=\"https://wandb.ai/benjikcf/AMEX\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/data_1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=4'>5</a>\u001b[0m callbacks\u001b[39m=\u001b[39m[ModelCheckpoint(dirpath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mckpt\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=5'>6</a>\u001b[0m                            monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_amex_metric\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=7'>8</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_epochs\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=8'>9</a>\u001b[0m                     logger\u001b[39m=\u001b[39mwandb_logger, callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=9'>10</a>\u001b[0m                     enable_progress_bar\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=11'>12</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdm)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=13'>14</a>\u001b[0m \u001b[39m# get validation metrics\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000009vscode-remote?line=14'>15</a>\u001b[0m val \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mvalidate(model, datamodule\u001b[39m=\u001b[39mdm, ckpt_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbest\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=750'>751</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=751'>752</a>\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=752'>753</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=766'>767</a>\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=767'>768</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=768'>769</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=769'>770</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=770'>771</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=771'>772</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:723\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=720'>721</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=721'>722</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=722'>723</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=723'>724</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=724'>725</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:811\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=806'>807</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=807'>808</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=808'>809</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=809'>810</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=810'>811</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=812'>813</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=813'>814</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1174\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1170'>1171</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39msetup_environment()\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1171'>1172</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__setup_profiler()\n\u001b[0;32m-> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1173'>1174</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_setup_hook()  \u001b[39m# allow user to setup lightning_module in accelerator environment\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1175'>1176</a>\u001b[0m \u001b[39m# check if we should delay restoring checkpoint till later\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1176'>1177</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mrestore_checkpoint_after_setup:\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1492\u001b[0m, in \u001b[0;36mTrainer._call_setup_hook\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1488'>1489</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mbarrier(\u001b[39m\"\u001b[39m\u001b[39mpre_setup\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1490'>1491</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatamodule \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1491'>1492</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatamodule\u001b[39m.\u001b[39;49msetup(stage\u001b[39m=\u001b[39;49mfn)\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1492'>1493</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39msetup\u001b[39m\u001b[39m\"\u001b[39m, stage\u001b[39m=\u001b[39mfn)\n\u001b[1;32m   <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py?line=1493'>1494</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39msetup\u001b[39m\u001b[39m\"\u001b[39m, stage\u001b[39m=\u001b[39mfn)\n",
      "\u001b[1;32m/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb Cell 4'\u001b[0m in \u001b[0;36mDataset_pl.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000003vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup\u001b[39m(\u001b[39mself\u001b[39m, stage\u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000003vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39m# Assign train/val datasets for use in dataloaders\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000003vscode-remote?line=10'>11</a>\u001b[0m     \u001b[39mif\u001b[39;00m stage \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m stage \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000003vscode-remote?line=11'>12</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_set \u001b[39m=\u001b[39m Dataset_AMEX(\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, fold\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfold)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000003vscode-remote?line=12'>13</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_set \u001b[39m=\u001b[39m Dataset_AMEX(\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m, fold\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfold)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_kaggle_ben.ipynb#ch0000003vscode-remote?line=13'>14</a>\u001b[0m     \u001b[39mif\u001b[39;00m stage \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalidate\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/code/amex/transformer/dataloader.py:14\u001b[0m, in \u001b[0;36mDataset_AMEX.__init__\u001b[0;34m(self, flag, fold)\u001b[0m\n\u001b[1;32m     <a href='file:///home/cairs/code/amex/transformer/dataloader.py?line=11'>12</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflag \u001b[39m=\u001b[39m flag\n\u001b[1;32m     <a href='file:///home/cairs/code/amex/transformer/dataloader.py?line=12'>13</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfold \u001b[39m=\u001b[39m fold\n\u001b[0;32m---> <a href='file:///home/cairs/code/amex/transformer/dataloader.py?line=13'>14</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__read_data__()\n",
      "File \u001b[0;32m~/code/amex/transformer/dataloader.py:24\u001b[0m, in \u001b[0;36mDataset_AMEX.__read_data__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/cairs/code/amex/transformer/dataloader.py?line=21'>22</a>\u001b[0m X_train \u001b[39m=\u001b[39m []; y_train \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='file:///home/cairs/code/amex/transformer/dataloader.py?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m train_idx:\n\u001b[0;32m---> <a href='file:///home/cairs/code/amex/transformer/dataloader.py?line=23'>24</a>\u001b[0m     X_train\u001b[39m.\u001b[39mappend( np\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mPATH_TO_DATA\u001b[39m}\u001b[39;49;00m\u001b[39mdata_\u001b[39;49m\u001b[39m{\u001b[39;49;00mk\u001b[39m}\u001b[39;49;00m\u001b[39m.npy\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     <a href='file:///home/cairs/code/amex/transformer/dataloader.py?line=24'>25</a>\u001b[0m     y_train\u001b[39m.\u001b[39mappend( pd\u001b[39m.\u001b[39mread_parquet(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPATH_TO_DATA\u001b[39m}\u001b[39;00m\u001b[39mtargets_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m.pqt\u001b[39m\u001b[39m'\u001b[39m) )\n\u001b[1;32m     <a href='file:///home/cairs/code/amex/transformer/dataloader.py?line=25'>26</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(X_train,axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/numpy/lib/npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/numpy/lib/npyio.py?line=387'>388</a>\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/numpy/lib/npyio.py?line=388'>389</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/numpy/lib/npyio.py?line=389'>390</a>\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/numpy/lib/npyio.py?line=390'>391</a>\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/numpy/lib/npyio.py?line=392'>393</a>\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/data_1.npy'"
     ]
    }
   ],
   "source": [
    "dm = Dataset_pl(1)\n",
    "model = Model_transformer()#, argv['batch_size']) \n",
    "\n",
    "wandb_logger = WandbLogger(project=\"AMEX\")\n",
    "callbacks=[ModelCheckpoint(dirpath='ckpt', \n",
    "                           monitor=\"val_amex_metric\", mode=\"max\")]\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=30, \n",
    "                    logger=wandb_logger, callbacks=callbacks,\n",
    "                    enable_progress_bar=False)\n",
    "\n",
    "trainer.fit(model, datamodule=dm)\n",
    "\n",
    "# get validation metrics\n",
    "val = trainer.validate(model, datamodule=dm, ckpt_path='best')\n",
    "val_amex_metric_epoch = val[0]['val_amex_metric_epoch']\n",
    "\n",
    "# get output\n",
    "output = trainer.predict(model, datamodule=dm, ckpt_path='best')\n",
    "output = torch.vstack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-combining",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training data shapes (367131, 13, 188) (367131,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type        | Params\n",
      "--------------------------------------------------\n",
      "0 | model             | Transformer | 509 K \n",
      "1 | train_amex_metric | AmexMetric  | 0     \n",
      "2 | val_amex_metric   | AmexMetric  | 0     \n",
      "3 | loss_fn           | BCELoss     | 0     \n",
      "--------------------------------------------------\n",
      "509 K     Trainable params\n",
      "0         Non-trainable params\n",
      "509 K     Total params\n",
      "2.037     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Validation data shapes (91782, 13, 188) (91782,)\n"
     ]
    }
   ],
   "source": [
    "# val_metrics = []\n",
    "# outputs = []\n",
    "# for i in range(1,11):\n",
    "#     dm = Dataset_pl(i)\n",
    "#     model = Model_transformer()#, argv['batch_size']) \n",
    "\n",
    "#     #wandb_logger = WandbLogger(project=\"AMEX\")\n",
    "#     callbacks=[ModelCheckpoint(dirpath='ckpt', \n",
    "#                                monitor=\"val_amex_metric\", mode=\"max\")]\n",
    "\n",
    "# #     trainer = pl.Trainer(gpus=[1], max_epochs=25, \n",
    "# #                         logger=wandb_logger, callbacks=callbacks,\n",
    "# #                         enable_progress_bar=False)\n",
    "#     trainer = pl.Trainer(gpus=[1], max_epochs=25, \n",
    "#                          callbacks=callbacks,\n",
    "#                          enable_progress_bar=False)\n",
    "#     trainer.fit(model, datamodule=dm)\n",
    "\n",
    "#     # get validation metrics\n",
    "#     val = trainer.validate(model, datamodule=dm, ckpt_path='best')\n",
    "#     val_amex_metric_epoch = val[0]['val_amex_metric_epoch']\n",
    "    \n",
    "#     # get output\n",
    "#     output = trainer.predict(model, datamodule=dm, ckpt_path='best')\n",
    "#     output = torch.vstack(output)\n",
    "#     # save result\n",
    "#     val_metrics.append(val_amex_metric_epoch)\n",
    "#     outputs.append(output)\n",
    "    \n",
    "#     print(f\"fold {i}\", val_amex_metric_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-dayton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-detector",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-programming",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-village",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-chess",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6bc16ffe0f94954532e9b31506694ff5e12654e3b81ed4ee0ecf18a2ec59813b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fft')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
