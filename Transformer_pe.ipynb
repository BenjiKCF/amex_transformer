{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "medieval-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ExponentialLR, CosineAnnealingLR, StepLR, OneCycleLR\n",
    "\n",
    "from dataloader import Dataset_AMEX\n",
    "from metric import AmexMetric\n",
    "from model_pe import Transformer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "consecutive-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Dataset_AMEX('val')\n",
    "# y_true = torch.tensor(dataset[0][1], dtype=torch.float)[None]\n",
    "\n",
    "# model = Transformer(num_tokens=1,\n",
    "#         feat_dim=188,\n",
    "#         embed_dim = 64,\n",
    "#         num_heads=4,\n",
    "#         num_encoder_layers=2,\n",
    "#         dropout_p=0.3)\n",
    "# y_hats = model(torch.tensor(dataset[0][0])[None])\n",
    "# loss_fn = nn.BCELoss(reduction=\"mean\")\n",
    "# loss_fn(y_hats.squeeze(1), y_true)\n",
    "# #val_amex_metric = AmexMetric()\n",
    "# #val_amex_metric.update(y_hats.reshape(-1), y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-budget",
   "metadata": {},
   "source": [
    "# Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "civilian-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_pl(pl.LightningDataModule):\n",
    "    def __init__(self, fold):\n",
    "        super().__init__()\n",
    "        self.fold = 1\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage= None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_set = Dataset_AMEX('train', fold=self.fold)\n",
    "            self.val_set = Dataset_AMEX('val', fold=self.fold)\n",
    "        if stage == \"validate\":\n",
    "            self.val_set = Dataset_AMEX('val', fold=self.fold)\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.val_set = Dataset_AMEX('val', fold=self.fold)\n",
    "        if stage == \"predict\" or stage is None:\n",
    "            self.test_set = Dataset_AMEX('test')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=512, shuffle=True, num_workers=1)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=2048, shuffle=False, num_workers=1)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=4096, shuffle=False, num_workers=1)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=4096, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-bathroom",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stupid-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_transformer(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3):#, batch_size):\n",
    "        super().__init__()\n",
    "        self.model = Transformer(num_tokens=1,\n",
    "                        feat_dim=188,\n",
    "                        embed_dim = 64,\n",
    "                        num_heads=4,\n",
    "                        num_encoder_layers=2,\n",
    "                        dropout_p=0.3)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_amex_metric = AmexMetric()\n",
    "        self.val_amex_metric = AmexMetric()\n",
    "        self.loss_fn = nn.BCELoss(reduction=\"mean\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        y_hat = self.model(x)\n",
    "        # loss function\n",
    "        loss = self.loss_fn(y_hat.squeeze(1), y)\n",
    "        self.train_amex_metric.update(y_hat.squeeze(1), y)\n",
    "        self.log_dict({'train_loss': loss, 'train_amex_metric': self.train_amex_metric}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        y_hat = self.model(x)\n",
    "        # loss function\n",
    "        loss = self.loss_fn(y_hat.squeeze(1), y)\n",
    "        self.val_amex_metric.update(y_hat.squeeze(1), y)\n",
    "        self.log_dict({'val_loss': loss, 'val_amex_metric': self.val_amex_metric}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss': loss}       \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        y_hat = self.model(x)\n",
    "        # loss function\n",
    "        #loss = self.loss_fn(y_hats.squeeze(1), y_true)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        x, y = x.float(), y.float()\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.model(x)#.squeeze(1)\n",
    "        return y_hat\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        lr_scheduler = OneCycleLR(optimizer, max_lr=1e-3, epochs=25, steps_per_epoch=718) #steps_per_epoch=len(dataloader)\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-tamil",
   "metadata": {},
   "source": [
    "# Find LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "refined-slave",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dm = Dataset_pl(1)\n",
    "# model = Model_transformer()#, argv['batch_size']) \n",
    "# trainer = pl.Trainer(gpus=2, strategy='dp')\n",
    "# lr_finder = trainer.tuner.lr_find(model, dm)\n",
    "\n",
    "# # Results can be found in\n",
    "# lr_finder.results\n",
    "\n",
    "# # Plot with\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()\n",
    "\n",
    "# # Pick point based on plot, or get suggestion\n",
    "# new_lr = lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-hammer",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "approved-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm = Dataset_pl(1)\n",
    "# model = Model_transformer()#, argv['batch_size']) \n",
    "\n",
    "# wandb_logger = WandbLogger(project=\"AMEX\")\n",
    "# callbacks=[ModelCheckpoint(dirpath='ckpt', \n",
    "#                            monitor=\"val_amex_metric\", mode=\"max\")]\n",
    "\n",
    "# trainer = pl.Trainer(gpus=[1], max_epochs=30, \n",
    "#                     logger=wandb_logger, callbacks=callbacks,\n",
    "#                     enable_progress_bar=False)\n",
    "\n",
    "# trainer.fit(model, datamodule=dm)\n",
    "\n",
    "# # get validation metrics\n",
    "# val = trainer.validate(model, datamodule=dm, ckpt_path='best')\n",
    "# val_amex_metric_epoch = val[0]['val_amex_metric_epoch']\n",
    "\n",
    "# # get output\n",
    "# output = trainer.predict(model, datamodule=dm, ckpt_path='best')\n",
    "# output = torch.vstack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "favorite-studio",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/cairs/code/amex/transformer/Transformer_pe.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000010vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m11\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000010vscode-remote?line=3'>4</a>\u001b[0m     dm \u001b[39m=\u001b[39m Dataset_pl(i)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000010vscode-remote?line=4'>5</a>\u001b[0m     model \u001b[39m=\u001b[39m Model_transformer()\u001b[39m#, argv['batch_size']) \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000010vscode-remote?line=6'>7</a>\u001b[0m     \u001b[39m#wandb_logger = WandbLogger(project=\"AMEX\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000010vscode-remote?line=7'>8</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[ModelCheckpoint(dirpath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mckpt\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000010vscode-remote?line=8'>9</a>\u001b[0m                                monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_amex_metric\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "\u001b[1;32m/home/cairs/code/amex/transformer/Transformer_pe.ipynb Cell 6'\u001b[0m in \u001b[0;36mModel_transformer.__init__\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000005vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m):\u001b[39m#, batch_size):\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000005vscode-remote?line=2'>3</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000005vscode-remote?line=3'>4</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m Transformer(num_tokens\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000005vscode-remote?line=4'>5</a>\u001b[0m                     feat_dim\u001b[39m=\u001b[39;49m\u001b[39m188\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000005vscode-remote?line=5'>6</a>\u001b[0m                     embed_dim \u001b[39m=\u001b[39;49m \u001b[39m64\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000005vscode-remote?line=6'>7</a>\u001b[0m                     num_heads\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000005vscode-remote?line=7'>8</a>\u001b[0m                     num_encoder_layers\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000005vscode-remote?line=8'>9</a>\u001b[0m                     dropout_p\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000005vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate \u001b[39m=\u001b[39m learning_rate\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e3131322e3139362e313230222c2275736572223a226361697273227d/home/cairs/code/amex/transformer/Transformer_pe.ipynb#ch0000005vscode-remote?line=10'>11</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_amex_metric \u001b[39m=\u001b[39m AmexMetric()\n",
      "File \u001b[0;32m~/code/amex/transformer/model_pe.py:122\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, num_tokens, embed_dim, feat_dim, num_heads, ff_dim, num_encoder_layers, dropout_p)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/code/amex/transformer/model_pe.py?line=119'>120</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings \u001b[39m=\u001b[39m {}\n\u001b[1;32m    <a href='file:///home/cairs/code/amex/transformer/model_pe.py?line=120'>121</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m11\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/cairs/code/amex/transformer/model_pe.py?line=121'>122</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings[k] \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mEmbedding(\u001b[39m10\u001b[39;49m, \u001b[39m4\u001b[39;49m)\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcuda:1\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/cairs/code/amex/transformer/model_pe.py?line=122'>123</a>\u001b[0m feat_dim2 \u001b[39m=\u001b[39m feat_dim\u001b[39m-\u001b[39m\u001b[39m11\u001b[39m\u001b[39m+\u001b[39m\u001b[39m11\u001b[39m\u001b[39m*\u001b[39m\u001b[39m4\u001b[39m\n\u001b[1;32m    <a href='file:///home/cairs/code/amex/transformer/model_pe.py?line=123'>124</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(feat_dim2, feat_dim) \u001b[39m# bs,13,feat_dim\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=922'>923</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=923'>924</a>\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=924'>925</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=926'>927</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=597'>598</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=598'>599</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=599'>600</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=600'>601</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=601'>602</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=602'>603</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=603'>604</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=921'>922</a>\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=922'>923</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=923'>924</a>\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> <a href='file:///home/cairs/miniconda3/envs/fft/lib/python3.9/site-packages/torch/nn/modules/module.py?line=924'>925</a>\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "val_metrics = []\n",
    "outputs = []\n",
    "for i in range(1,11):\n",
    "    dm = Dataset_pl(i)\n",
    "    model = Model_transformer()#, argv['batch_size']) \n",
    "\n",
    "    #wandb_logger = WandbLogger(project=\"AMEX\")\n",
    "    callbacks=[ModelCheckpoint(dirpath='ckpt', \n",
    "                               monitor=\"val_amex_metric\", mode=\"max\")]\n",
    "\n",
    "#     trainer = pl.Trainer(gpus=[1], max_epochs=25, \n",
    "#                         logger=wandb_logger, callbacks=callbacks,\n",
    "#                         enable_progress_bar=False)\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=25, \n",
    "                         callbacks=callbacks,\n",
    "                         enable_progress_bar=True)\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "\n",
    "    # get validation metrics\n",
    "    val = trainer.validate(model, datamodule=dm, ckpt_path='best')\n",
    "    val_amex_metric_epoch = val[0]['val_amex_metric_epoch']\n",
    "    \n",
    "    # get output\n",
    "    output = trainer.predict(model, datamodule=dm, ckpt_path='best')\n",
    "    output = torch.vstack(output)\n",
    "    # save result\n",
    "    val_metrics.append(val_amex_metric_epoch)\n",
    "    outputs.append(output)\n",
    "    \n",
    "    print(f\"fold {i}\", val_amex_metric_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-ready",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-kansas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-laundry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-chemical",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-links",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-double",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-warning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6bc16ffe0f94954532e9b31506694ff5e12654e3b81ed4ee0ecf18a2ec59813b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fft')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
